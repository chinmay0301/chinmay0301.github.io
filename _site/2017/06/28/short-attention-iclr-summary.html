<!DOCTYPE html>
<html>

  <head>
    <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Frustratingly Short Attention Spans (ICLR 2017) - A Summary</title>
  <meta name="description" content="Through this blogpost I attempt to summarise the key ideas highlighted in an ICLR-2017 accepted paper, Frustratingly Short Attention Spans in Neural Language...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/2017/06/28/short-attention-iclr-summary.html">
  <link rel="alternate" type="application/rss+xml" title="Chinmay Talegaonkar" href="http://localhost:4000/feed.xml">
</head>

    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>


    <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Frustratingly Short Attention Spans (ICLR 2017) - A Summary | Chinmay Talegaonkar</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Frustratingly Short Attention Spans (ICLR 2017) - A Summary" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Through this blogpost I attempt to summarise the key ideas highlighted in an ICLR-2017 accepted paper, Frustratingly Short Attention Spans in Neural Language Modelling, (read it here), by Daniluk et al, of the University College London. You can read the official ICLR reviews on OpenReview." />
<meta property="og:description" content="Through this blogpost I attempt to summarise the key ideas highlighted in an ICLR-2017 accepted paper, Frustratingly Short Attention Spans in Neural Language Modelling, (read it here), by Daniluk et al, of the University College London. You can read the official ICLR reviews on OpenReview." />
<link rel="canonical" href="http://localhost:4000/2017/06/28/short-attention-iclr-summary.html" />
<meta property="og:url" content="http://localhost:4000/2017/06/28/short-attention-iclr-summary.html" />
<meta property="og:site_name" content="Chinmay Talegaonkar" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2017-06-28T10:00:55+05:30" />
<script type="application/ld+json">
{"url":"http://localhost:4000/2017/06/28/short-attention-iclr-summary.html","headline":"Frustratingly Short Attention Spans (ICLR 2017) - A Summary","dateModified":"2017-06-28T10:00:55+05:30","datePublished":"2017-06-28T10:00:55+05:30","description":"Through this blogpost I attempt to summarise the key ideas highlighted in an ICLR-2017 accepted paper, Frustratingly Short Attention Spans in Neural Language Modelling, (read it here), by Daniluk et al, of the University College London. You can read the official ICLR reviews on OpenReview.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2017/06/28/short-attention-iclr-summary.html"},"@type":"BlogPosting","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    
      <meta content="https://avatars1.githubusercontent.com/u/8805240" property="og:image" />
    
  </head>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Chinmay Talegaonkar</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
          <a class="page-link" href="/archive.html">Blog</a>
          
        
          
          <a class="page-link" href="/cv/">CV</a>
          
        
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/projects/">Projects</a>
          
        
          
          <a class="page-link" href="/research/">Research</a>
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Frustratingly Short Attention Spans (ICLR 2017) - A Summary</h1>
    <p class="post-meta"><time datetime="2017-06-28T10:00:55+05:30" itemprop="datePublished">Jun 28, 2017</time>  • <a href="https://sgeos.github.io/2017/06/28/short-attention-iclr-summary.html#disqus_thread">0 Comments</a></p>
    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
>
</script>
<script
  type="text/javascript"
  charset="utf-8"
  src="https://vincenttam.github.io/javascripts/MathJaxLocal.js"
>
</script>


  </header>

  <div class="post-content" itemprop="articleBody">
    <p>Through this blogpost I attempt to summarise the key ideas highlighted in an <a href="http://www.iclr.cc/doku.php?id=ICLR2017:main&amp;redirect=1">ICLR-2017</a> accepted paper, Frustratingly Short Attention Spans in Neural Language Modelling, (read it <a href="https://arxiv.org/abs/1702.04521">here</a>), by <a href="https://www.linkedin.com/in/michaldaniluk91/?ppe=1">Daniluk</a> et al, of the University College London. You can read the official ICLR reviews on <a href="https://openreview.net/forum?id=ByIAPUcee">OpenReview</a>.</p>

<p>In my opinion, this is a very well written paper with strongly motivated ideas and strong results. The original manuscript explains the ideas well, and I hope that this blogpost does justice to the paper!</p>

<h2 id="motivation">Motivation</h2>

<p>This paper is primarily motivated by the shortcomings of the traditional attention mechanism proposed in the <a href="https://arxiv.org/pdf/1409.0473.pdf">Bahdanau et al.</a> ICLR-2015 paper. It tries to improve the traditional attention mechanism with a couple of more meaningful models in the context of <a href="https://en.wikipedia.org/wiki/Language_model">language modelling</a>.
While implementing their new architecture, the authors notice a strange trend in the lengths of the attention spans (and hence the title) which motivates them to build a simple novel architecture, the “N-Gram Recurrent Neural Network”, which achieves comparable performance to the more complicated attention models.</p>

<p>Hence, this paper ends by questioning the usefulness of complicated attention models, and tries to highlight that today’s state-of-the-art architectures suffer from a long term dependency issue, especially in language modelling.</p>

<h2 id="traditional-attention-models">Traditional Attention Models</h2>

<p>The main issue with traditional attention is evident from the equations, they try to squeeze three pieces of information in a single vector. (If you haven’t heard about attention models, you can head over to colah’s <a href="http://distill.pub/2016/augmented-rnns/">blog</a>). As depicted in the figure below, the paper assumes a fixed length attention span.</p>

<p><img src="http://localhost:4000/assets/model1.png" alt="model1" /></p>

<p><strong>Notation</strong> - $x_t$ depicts input vectors, $c_t$ depicts the LSTM hidden vector, $h_t$ depicts the LSTM output vectors, $r$ contains the attention information and $h^*$ represents the final output vector (after attending to previous $h$ units).</p>

<p>Pay close attention to $\color{blue}{h_i}$ in the following equations,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} \textbf{M}_t & = \tanh(\textbf{W}^Y[\color{blue}{h_{t-L}} ... \color{blue}{h_{t-1}}] + \textbf{W}^h[\color{blue}{h_t} ... \color{blue}{h_t}]) \\ \pmb{\alpha}_t & = softmax(\textbf{w}^T\textbf{M}_t) \\ \textbf{r}_t & = [\color{blue}{h_{t-L}} ... \color{blue}{h_{t-1}}]\pmb{\alpha}^T \\ \textbf{h}^{*}_t & = \tanh(\textbf{W}^r\textbf{r}_t + \textbf{W}^x\color{blue}{h_t}) \end{align} %]]></script>

<p>In other words,<br />
<em>(Compares each $h_{t-i}$ with $h_t$, using weights $\textbf{W}^{Y}$ and $\textbf{W}^h$)</em><br />
<em>(Compute attention weights according to previous comparison)</em><br />
<em>(Weight $h_{t-i}$ according to computed attention weights)</em><br />
<em>(Compute final vector using attention information and current output vector)</em></p>

<p>Quite clearly, the output vectors $\color{blue}{h_i}$ are being used in a three-fold role -</p>
<ul>
  <li><span style="color:red"><strong>Keys</strong></span> for comparison while computing attention vector.</li>
  <li><span style="color:blue"><strong>Values</strong></span> for computing $\textbf{r}_t$ from attention weights.</li>
  <li>Encode distribution for <span style="color:green"><strong>prediction</strong></span> of next token.</li>
</ul>

<p>This motivates the <em>key-value</em> and the <em>key-value-prediction</em> model, which attempts to use different vectors (all output vectors of the LSTM) for each of the three roles.</p>

<h2 id="new-attention-models">New Attention Models</h2>

<p>As depicted in Figure 1(b) and 1(c) of the <a href="https://arxiv.org/abs/1702.04521">paper</a>, the LSTM produces multiple output vectors (or equivalently, divides $h_t$ into two or more parts). More concretely,</p>

<ul>
  <li><strong><span style="color:red">Key</span>-<span style="color:blue">Value</span> Attention</strong> - A different vector is used to compute the attention vector. However, the same vector is still used for the values and predictions.</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} \textbf{M}_t & = \tanh(\textbf{W}^Y[\color{red}{k_{t-L}} ... \color{red}{k_{t-1}}] + \textbf{W}^h[\color{red}{k_t} ... \color{red}{k_t}]) \\ \pmb{\alpha}_t & = softmax(\textbf{w}^T\textbf{M}_t) \\ \textbf{r}_t & = [\color{blue}{v_{t-L}} ... \color{blue}{v_{t-1}}]\pmb{\alpha}^T \\ \textbf{h}^{*}_t & = \tanh(\textbf{W}^r\textbf{r}_t + \textbf{W}^x\color{blue}{v_t}) \end{align} %]]></script>

<ul>
  <li><strong><span style="color:red">Key</span>-<span style="color:blue">Value</span>-<span style="color:green">Prediction</span> Attention</strong> - A different vector is used for each of the three roles described above. More specifically,</li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} \textbf{M}_t & = \tanh(\textbf{W}^Y[\color{red}{k_{t-L}} ... \color{red}{k_{t-1}}] + \textbf{W}^h[\color{red}{k_t} ... \color{red}{k_t}]) \\ \pmb{\alpha}_t & = softmax(\textbf{w}^T\textbf{M}_t) \\ \textbf{r}_t & = [\color{blue}{v_{t-L}} ... \color{blue}{v_{t-1}}]\pmb{\alpha}^T \\ \textbf{h}^{*}_t & = \tanh(\textbf{W}^r\textbf{r}_t + \textbf{W}^x\color{green}{p_t}) \end{align} %]]></script>

<p>The different hidden vector dimensions were chosen to keep <strong>identical number of trainable variables</strong>.  As expected, in nearly all the experiments there is an increasingly better performance from <em>traditional attention</em>, to <em>key-value attention</em>, to <em>key-value-prediction attention</em>. However, the attention visualizations of the model showed a different story.</p>

<h2 id="n-gram-recurrent-neural-networks">N-Gram Recurrent Neural Networks</h2>

<p>Figure 3(a) and Figure 3(b) of the paper show an interesting trend, and it seems like most of the attention is focussed on the previous 2-5 outputs only. The results of Figure 2(a) show similar perplexities across different attention sizes.</p>

<p><img src="http://localhost:4000/assets/model2.png" alt="model2" /></p>

<p>This finding motivates the <em>$N$-Gram RNN</em>, a simple structure which <strong>always</strong> attends over the previous $N-1$ outputs. This is similar to $N$-gram language models, (<a href="https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf?sequence=1">Chen and Goodman</a>), which use the previous $N-1$ tokens (a <em>context</em>) to predict the next token. More specifically,</p>

<p><img src="http://localhost:4000/assets/model3.png" alt="model3" /></p>

<p>The LSTM outputs $N-1$ vectors (or equivalently divides $\color{blue}{h_{t}}$ into $N-1$ parts, each of which encode information to help predict the $i^{th}$ (from $1$ to $N-1$) token of the future. It’s a very simple neural network, and the four equations in each of the above models are replaced by,</p>

<script type="math/tex; mode=display">\textbf{h}^{*}_{t} = \tanh(\textbf{W}^{N}[\color{blue}{h_{t}^{1}}...\color{red}{h_{t-i+1}^{i}} ... \color{green}{h_{t-N+2}^{N-1} }]^T), i \in \{1, 2, ... N-1\}</script>

<p>Quite strangely, <em>4-gram networks</em> perform nearly as well as <em>key-value-predict</em> models (see Figure 2(c)). Of course, this might be very specific to language modelling, but perhaps attention needs a re-thinking?</p>

<h2 id="openreview-summaries">OpenReview Summaries</h2>
<p>This paper received a 7/10 in ICLR-2017 and was accepted for a poster presentation. Here were the major highlights of the <a href="https://openreview.net/forum?id=ByIAPUcee">reviews</a> -</p>

<ul>
  <li>
    <p>There was some concern about the choice of hyperparameters. The <em>key-value</em> and <em>key-value-predict</em> models were attending over much larger vectors than the traditional models. However, the total number of weights had been adjusted to be uniform across models. There was also some concern over the choice of an attention span of just 15 and an unrolling of just 20 timesteps. The authors do report better results on using <a href="https://en.wikipedia.org/wiki/Backpropagation_through_time">BPTT</a> through 35 timesteps.</p>
  </li>
  <li>
    <p>There was a general consensus that an impactful corpus (the Wikipedia corpus) for long-term language modelling had been released, since several dependencies were separated by dozens of timesteps, making it harder for attention based models to capture them.</p>
  </li>
  <li>
    <p>There was a general consensus that a very thorough experimentation was conducted. The paper was well explained and they liked the idea of the $N$-Gram RNN baseline. The paper raises important questions about attention in neural networks.</p>
  </li>
</ul>

<h2 id="clarifications-from-authors">Clarifications from Authors</h2>
<p>I contacted the authors to clarify a few questions raised during the reading group meeting at <a href="http://ttic.uchicago.edu/~klivescu/SLATTIC/">SLATTIC</a>. Here’s a brief summary of their reply,</p>

<ul>
  <li><strong>BPTT</strong> - There was a concern about the exact mechanism used for Back Propagation Through Time, since the unrolling was done only through a fixed number of timesteps. The input tensors have been broken into chunks of size <code class="highlighter-rouge">batch * bptt_steps * embedding_size</code>, which is used for all gradient calculations in that timestep. After a training step, the final LSTM hidden vectors produced are passed over as the initial hidden vectors for the next training step. (Unless there is a article break). Except this hidden vector, no information about the previous chunks are used during training.
Higher <code class="highlighter-rouge">bptt_steps</code> values were tried, and produced marginal improvement in results, but the GPU memory constraints limited their experimentation.</li>
  <li><strong>Short Attention Spans</strong> - Figure 2(a) is a surprising set of results. In the author’s words, <em>“We found that very surprising too. One criticism of our work is that the corpora we trained on might not really need long-range dependencies to learn a good language model – or in other words, cases where such long-range dependencies are really needed are so rare that they don’t provide enough training signal to actually learn a sensible attention mechanism. Check out this thread for the discussion - <a href="https://twitter.com/tallinzen/status/832174893152219136">Twitter</a>.</em>”</li>
</ul>

<h2 id="personal-opinions">Personal Opinions</h2>
<p>I personally agree with the comments on OpenReview and the conclusions drawn by the author, and they have done a great job with the explanations in the paper. I’m a bit skeptical about Figure 2(a), since I did not expect such a <em>small</em> variation across attention sizes. A window size of 1 is doing nearly as well as sizes of 5, 10 and 15, which puts a little doubt in my mind about the system. Nevertheless, I think this is great research work since,</p>

<ul>
  <li>
    <p><strong>Raises Important Questions</strong> - Instead of trying to beat state-of-the-art, this paper raises an important question - Long term dependencies are far from solved. I think this is a wake-up call for the larger companies that simpler / smarter architectures need to be discovered, and larger / deeper networks aren’t always the solution. The future work sounds really promising, and it would be great to see architectures that force models to ignore local context.</p>
  </li>
  <li>
    <p><strong>Great / Fair Comparisons</strong> - I think a good amount of care has been taken to ensure fair comparison between models. The rebuttal on <a href="https://openreview.net/forum?id=ByIAPUcee">OpenReview</a> supports this.</p>
  </li>
  <li>
    <p><strong>Simpler is Better</strong> - Perhaps the part I liked best in this paper is the simplicity of their $N$-gram network idea. This is one of the first papers I’ve seen that tries to match its own model with a baseline model, raising crucial questions.</p>
  </li>
</ul>

  </div>

</article>


<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://http-martiansideofthemoon-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Chinmay Talegaonkar</li>
          <li><a href="mailto:chinmay0301@gmail.com">chinmay0301@gmail.com</a></li>
          <li><a href="mailto:chinmay0301@iitb.ac.in">chinmay0301@iitb.ac.in</a></li>
        </ul>
      </div>
      <div class="footer-col footer-col-1">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/chinmay0301"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">chinmay0301</span></a>

          </li>
          
          
          
          <li>
            <a href="https://www.linkedin.com/in/chinmay0301"><span class="icon icon--linkedin"><svg  x="0px" y="0px" width="16px" height="16px" viewBox="0 50 512 512" style="enable-background:new 0 0 90 90;" xml:space="preserve">
<g>
    <path fill="#828282" d="M150.65,100.682c0,27.992-22.508,50.683-50.273,50.683c-27.765,0-50.273-22.691-50.273-50.683
    C50.104,72.691,72.612,50,100.377,50C128.143,50,150.65,72.691,150.65,100.682z M143.294,187.333H58.277V462h85.017V187.333z
    M279.195,187.333h-81.541V462h81.541c0,0,0-101.877,0-144.181c0-38.624,17.779-61.615,51.807-61.615
    c31.268,0,46.289,22.071,46.289,61.615c0,39.545,0,144.181,0,144.181h84.605c0,0,0-100.344,0-173.915
    s-41.689-109.131-99.934-109.131s-82.768,45.369-82.768,45.369V187.333z"/>
</g>
</svg></span><span class="username">&nbsp;Chinmay Talegaonkar</span></a>

          </li>
          
        </ul>
      </div>
      <div class="footer-col footer-col-4">
        <!-- <p></p> -->
        <ul class="social-media-list">
        
        </ul>
      </div>
      
      
      
    </div>

  </div>

</footer>

    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-89391111-1', 'auto');
  ga('send', 'pageview');

</script>
    <script id="dsq-count-scr" src="//http-martiansideofthemoon-github-io.disqus.com/count.js" async></script>
  </body>

</html>
